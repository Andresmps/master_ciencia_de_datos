{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Spark"
      ],
      "metadata": {
        "id": "Qgaeck94a1jM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tQq08nU6RtS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5cdae0e-1098-4b2c-f8d4-c490ed833640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "25 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.4.2/spark-3.4.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.2-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "63YwHzxUczK-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1g18PZhdUO8",
        "outputId": "d616fccc-7bbb-4671-ba2b-560901d97f35"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.2-bin-hadoop3\"\n",
        "\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SWc6DktwdWFl",
        "outputId": "dca35123-089d-4a4b-a968-489f8ff8981f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.4.2-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"actividad2-spark\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "g86kFHnjcrDt",
        "outputId": "ae69a342-bed7-4960-aa21-f1718f9c3ead"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f6b2415aa70>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://03d6364d9dd2:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VatapO0TcrMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad 2\n",
        "---"
      ],
      "metadata": {
        "id": "At82KKPRbITu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punto 1"
      ],
      "metadata": {
        "id": "LcxiMMTEeNx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"actividad2/punto1/data/output/\"\n",
        "!$SPARK_HOME/bin/spark-submit \\\n",
        "    ./actividad2/punto1/src/spark-job.py \\\n",
        "    ./actividad2/punto1/data/input.txt \\\n",
        "    ./actividad2/punto1/data/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWOxAuHIpvXl",
        "outputId": "2b770546-895f-4c86-9235-102a35f40ab3"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/01/15 21:56:48 INFO SparkContext: Running Spark version 3.4.2\n",
            "24/01/15 21:56:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/01/15 21:56:49 INFO ResourceUtils: ==============================================================\n",
            "24/01/15 21:56:49 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/01/15 21:56:49 INFO ResourceUtils: ==============================================================\n",
            "24/01/15 21:56:49 INFO SparkContext: Submitted application: personaGastosConTarjetaCredito\n",
            "24/01/15 21:56:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/01/15 21:56:49 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/01/15 21:56:49 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/01/15 21:56:49 INFO SecurityManager: Changing view acls to: root\n",
            "24/01/15 21:56:49 INFO SecurityManager: Changing modify acls to: root\n",
            "24/01/15 21:56:49 INFO SecurityManager: Changing view acls groups to: \n",
            "24/01/15 21:56:49 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/01/15 21:56:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/01/15 21:56:49 INFO Utils: Successfully started service 'sparkDriver' on port 46109.\n",
            "24/01/15 21:56:49 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/01/15 21:56:49 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/01/15 21:56:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/01/15 21:56:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/01/15 21:56:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/01/15 21:56:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-249d29f8-3b00-4675-9b9b-411480432791\n",
            "24/01/15 21:56:50 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "24/01/15 21:56:50 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/01/15 21:56:50 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/01/15 21:56:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "24/01/15 21:56:50 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "24/01/15 21:56:50 INFO Executor: Starting executor ID driver on host 03d6364d9dd2\n",
            "24/01/15 21:56:50 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/01/15 21:56:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44723.\n",
            "24/01/15 21:56:50 INFO NettyBlockTransferService: Server created on 03d6364d9dd2:44723\n",
            "24/01/15 21:56:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/01/15 21:56:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 03d6364d9dd2, 44723, None)\n",
            "24/01/15 21:56:50 INFO BlockManagerMasterEndpoint: Registering block manager 03d6364d9dd2:44723 with 366.3 MiB RAM, BlockManagerId(driver, 03d6364d9dd2, 44723, None)\n",
            "24/01/15 21:56:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 03d6364d9dd2, 44723, None)\n",
            "24/01/15 21:56:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 03d6364d9dd2, 44723, None)\n",
            "24/01/15 21:56:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 358.0 KiB, free 366.0 MiB)\n",
            "24/01/15 21:56:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.9 MiB)\n",
            "24/01/15 21:56:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 03d6364d9dd2:44723 (size: 32.5 KiB, free: 366.3 MiB)\n",
            "24/01/15 21:56:53 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "24/01/15 21:56:54 INFO FileInputFormat: Total input files to process : 1\n",
            "24/01/15 21:56:54 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "24/01/15 21:56:54 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:56:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:56:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:56:55 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "24/01/15 21:56:55 INFO DAGScheduler: Registering RDD 3 (distinct at /content/actividad2/punto1/src/spark-job.py:21) as input to shuffle 1\n",
            "24/01/15 21:56:55 INFO DAGScheduler: Registering RDD 10 (reduceByKey at /content/actividad2/punto1/src/spark-job.py:32) as input to shuffle 0\n",
            "24/01/15 21:56:55 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 4 output partitions\n",
            "24/01/15 21:56:55 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at SparkHadoopWriter.scala:83)\n",
            "24/01/15 21:56:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
            "24/01/15 21:56:55 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
            "24/01/15 21:56:55 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at distinct at /content/actividad2/punto1/src/spark-job.py:21), which has no missing parents\n",
            "24/01/15 21:56:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)\n",
            "24/01/15 21:56:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.9 MiB)\n",
            "24/01/15 21:56:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 03d6364d9dd2:44723 (size: 7.5 KiB, free: 366.3 MiB)\n",
            "24/01/15 21:56:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "24/01/15 21:56:55 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at distinct at /content/actividad2/punto1/src/spark-job.py:21) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/01/15 21:56:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "24/01/15 21:56:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (03d6364d9dd2, executor driver, partition 0, PROCESS_LOCAL, 7414 bytes) \n",
            "24/01/15 21:56:55 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (03d6364d9dd2, executor driver, partition 1, PROCESS_LOCAL, 7414 bytes) \n",
            "24/01/15 21:56:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "24/01/15 21:56:55 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "24/01/15 21:56:56 INFO HadoopRDD: Input split: file:/content/actividad2/punto1/data/input.txt:59+60\n",
            "24/01/15 21:56:56 INFO HadoopRDD: Input split: file:/content/actividad2/punto1/data/input.txt:0+59\n",
            "24/01/15 21:56:58 INFO PythonRunner: Times: total = 1089, boot = 851, init = 237, finish = 1\n",
            "24/01/15 21:56:58 INFO PythonRunner: Times: total = 1090, boot = 864, init = 225, finish = 1\n",
            "24/01/15 21:56:58 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1624 bytes result sent to driver\n",
            "24/01/15 21:56:58 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1624 bytes result sent to driver\n",
            "24/01/15 21:56:58 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2447 ms on 03d6364d9dd2 (executor driver) (1/2)\n",
            "24/01/15 21:56:58 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 34887\n",
            "24/01/15 21:56:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2508 ms on 03d6364d9dd2 (executor driver) (2/2)\n",
            "24/01/15 21:56:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/01/15 21:56:58 INFO DAGScheduler: ShuffleMapStage 0 (distinct at /content/actividad2/punto1/src/spark-job.py:21) finished in 2.830 s\n",
            "24/01/15 21:56:58 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/01/15 21:56:58 INFO DAGScheduler: running: Set()\n",
            "24/01/15 21:56:58 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)\n",
            "24/01/15 21:56:58 INFO DAGScheduler: failed: Set()\n",
            "24/01/15 21:56:58 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[10] at reduceByKey at /content/actividad2/punto1/src/spark-job.py:32), which has no missing parents\n",
            "24/01/15 21:56:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 19.0 KiB, free 365.9 MiB)\n",
            "24/01/15 21:56:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 9.3 KiB, free 365.9 MiB)\n",
            "24/01/15 21:56:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 03d6364d9dd2:44723 (size: 9.3 KiB, free: 366.3 MiB)\n",
            "24/01/15 21:56:58 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "24/01/15 21:56:58 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 1 (PairwiseRDD[10] at reduceByKey at /content/actividad2/punto1/src/spark-job.py:32) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "24/01/15 21:56:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0\n",
            "24/01/15 21:56:58 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 2) (03d6364d9dd2, executor driver, partition 3, NODE_LOCAL, 7279 bytes) \n",
            "24/01/15 21:56:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3) (03d6364d9dd2, executor driver, partition 0, PROCESS_LOCAL, 7523 bytes) \n",
            "24/01/15 21:56:58 INFO Executor: Running task 3.0 in stage 1.0 (TID 2)\n",
            "24/01/15 21:56:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 3)\n",
            "24/01/15 21:56:58 INFO HadoopRDD: Input split: file:/content/actividad2/punto1/data/input.txt:0+59\n",
            "24/01/15 21:56:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 03d6364d9dd2:44723 in memory (size: 7.5 KiB, free: 366.3 MiB)\n",
            "24/01/15 21:56:58 INFO ShuffleBlockFetcherIterator: Getting 2 (177.0 B) non-empty blocks including 2 (177.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:56:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 78 ms\n",
            "24/01/15 21:56:58 INFO PythonRunner: Times: total = 382, boot = -795, init = 1177, finish = 0\n",
            "24/01/15 21:56:59 INFO PythonRunner: Times: total = 350, boot = -851, init = 1200, finish = 1\n",
            "24/01/15 21:56:59 INFO Executor: Finished task 0.0 in stage 1.0 (TID 3). 1626 bytes result sent to driver\n",
            "24/01/15 21:56:59 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4) (03d6364d9dd2, executor driver, partition 1, PROCESS_LOCAL, 7523 bytes) \n",
            "24/01/15 21:56:59 INFO Executor: Running task 1.0 in stage 1.0 (TID 4)\n",
            "24/01/15 21:56:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 538 ms on 03d6364d9dd2 (executor driver) (1/4)\n",
            "24/01/15 21:56:59 INFO HadoopRDD: Input split: file:/content/actividad2/punto1/data/input.txt:59+60\n",
            "24/01/15 21:56:59 INFO PythonRunner: Times: total = 422, boot = 13, init = 407, finish = 2\n",
            "24/01/15 21:56:59 INFO PythonRunner: Times: total = 491, boot = 21, init = 462, finish = 8\n",
            "24/01/15 21:56:59 INFO Executor: Finished task 3.0 in stage 1.0 (TID 2). 2314 bytes result sent to driver\n",
            "24/01/15 21:56:59 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5) (03d6364d9dd2, executor driver, partition 2, PROCESS_LOCAL, 7279 bytes) \n",
            "24/01/15 21:56:59 INFO Executor: Running task 2.0 in stage 1.0 (TID 5)\n",
            "24/01/15 21:56:59 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 2) in 962 ms on 03d6364d9dd2 (executor driver) (2/4)\n",
            "24/01/15 21:56:59 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "24/01/15 21:56:59 INFO PythonRunner: Times: total = 466, boot = -84, init = 550, finish = 0\n",
            "24/01/15 21:56:59 INFO PythonRunner: Times: total = 422, boot = -139, init = 561, finish = 0\n",
            "24/01/15 21:56:59 INFO Executor: Finished task 1.0 in stage 1.0 (TID 4). 1583 bytes result sent to driver\n",
            "24/01/15 21:56:59 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 556 ms on 03d6364d9dd2 (executor driver) (3/4)\n",
            "24/01/15 21:56:59 INFO PythonRunner: Times: total = 247, boot = -152, init = 399, finish = 0\n",
            "24/01/15 21:56:59 INFO PythonRunner: Times: total = 264, boot = -15, init = 278, finish = 1\n",
            "24/01/15 21:56:59 INFO Executor: Finished task 2.0 in stage 1.0 (TID 5). 2099 bytes result sent to driver\n",
            "24/01/15 21:56:59 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 335 ms on 03d6364d9dd2 (executor driver) (4/4)\n",
            "24/01/15 21:56:59 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "24/01/15 21:56:59 INFO DAGScheduler: ShuffleMapStage 1 (reduceByKey at /content/actividad2/punto1/src/spark-job.py:32) finished in 1.343 s\n",
            "24/01/15 21:56:59 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/01/15 21:56:59 INFO DAGScheduler: running: Set()\n",
            "24/01/15 21:56:59 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
            "24/01/15 21:56:59 INFO DAGScheduler: failed: Set()\n",
            "24/01/15 21:56:59 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "24/01/15 21:56:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 109.0 KiB, free 365.8 MiB)\n",
            "24/01/15 21:56:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 41.3 KiB, free 365.7 MiB)\n",
            "24/01/15 21:56:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 03d6364d9dd2:44723 (size: 41.3 KiB, free: 366.2 MiB)\n",
            "24/01/15 21:56:59 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "24/01/15 21:56:59 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "24/01/15 21:56:59 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks resource profile 0\n",
            "24/01/15 21:56:59 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6) (03d6364d9dd2, executor driver, partition 1, NODE_LOCAL, 7181 bytes) \n",
            "24/01/15 21:56:59 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 7) (03d6364d9dd2, executor driver, partition 3, NODE_LOCAL, 7181 bytes) \n",
            "24/01/15 21:56:59 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)\n",
            "24/01/15 21:56:59 INFO Executor: Running task 3.0 in stage 2.0 (TID 7)\n",
            "24/01/15 21:56:59 INFO ShuffleBlockFetcherIterator: Getting 2 (185.0 B) non-empty blocks including 2 (185.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:56:59 INFO ShuffleBlockFetcherIterator: Getting 2 (168.0 B) non-empty blocks including 2 (168.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "24/01/15 21:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "24/01/15 21:57:00 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:57:00 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:57:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:57:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:57:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:57:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:57:00 INFO PythonRunner: Times: total = 264, boot = -370, init = 634, finish = 0\n",
            "24/01/15 21:57:00 INFO PythonRunner: Times: total = 280, boot = -363, init = 642, finish = 1\n",
            "24/01/15 21:57:00 INFO FileOutputCommitter: Saved output of task 'attempt_202401152156542740283994940460430_0015_m_000003_0' to file:/content/actividad2/punto1/data/output/_temporary/0/task_202401152156542740283994940460430_0015_m_000003\n",
            "24/01/15 21:57:00 INFO SparkHadoopMapRedUtil: attempt_202401152156542740283994940460430_0015_m_000003_0: Committed. Elapsed time: 7 ms.\n",
            "24/01/15 21:57:00 INFO FileOutputCommitter: Saved output of task 'attempt_202401152156542740283994940460430_0015_m_000001_0' to file:/content/actividad2/punto1/data/output/_temporary/0/task_202401152156542740283994940460430_0015_m_000001\n",
            "24/01/15 21:57:00 INFO SparkHadoopMapRedUtil: attempt_202401152156542740283994940460430_0015_m_000001_0: Committed. Elapsed time: 6 ms.\n",
            "24/01/15 21:57:00 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 2351 bytes result sent to driver\n",
            "24/01/15 21:57:00 INFO Executor: Finished task 3.0 in stage 2.0 (TID 7). 2394 bytes result sent to driver\n",
            "24/01/15 21:57:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8) (03d6364d9dd2, executor driver, partition 0, PROCESS_LOCAL, 7181 bytes) \n",
            "24/01/15 21:57:00 INFO Executor: Running task 0.0 in stage 2.0 (TID 8)\n",
            "24/01/15 21:57:00 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 7) in 423 ms on 03d6364d9dd2 (executor driver) (1/4)\n",
            "24/01/15 21:57:00 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 9) (03d6364d9dd2, executor driver, partition 2, PROCESS_LOCAL, 7181 bytes) \n",
            "24/01/15 21:57:00 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 432 ms on 03d6364d9dd2 (executor driver) (2/4)\n",
            "24/01/15 21:57:00 INFO Executor: Running task 2.0 in stage 2.0 (TID 9)\n",
            "24/01/15 21:57:00 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:57:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/01/15 21:57:00 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:57:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "24/01/15 21:57:00 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:57:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:57:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:57:00 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:57:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:57:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:57:00 INFO PythonRunner: Times: total = 215, boot = -573, init = 787, finish = 1\n",
            "24/01/15 21:57:00 INFO FileOutputCommitter: Saved output of task 'attempt_202401152156542740283994940460430_0015_m_000000_0' to file:/content/actividad2/punto1/data/output/_temporary/0/task_202401152156542740283994940460430_0015_m_000000\n",
            "24/01/15 21:57:00 INFO SparkHadoopMapRedUtil: attempt_202401152156542740283994940460430_0015_m_000000_0: Committed. Elapsed time: 4 ms.\n",
            "24/01/15 21:57:00 INFO Executor: Finished task 0.0 in stage 2.0 (TID 8). 2265 bytes result sent to driver\n",
            "24/01/15 21:57:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 289 ms on 03d6364d9dd2 (executor driver) (3/4)\n",
            "24/01/15 21:57:00 INFO PythonRunner: Times: total = 310, boot = -532, init = 842, finish = 0\n",
            "24/01/15 21:57:00 INFO FileOutputCommitter: Saved output of task 'attempt_202401152156542740283994940460430_0015_m_000002_0' to file:/content/actividad2/punto1/data/output/_temporary/0/task_202401152156542740283994940460430_0015_m_000002\n",
            "24/01/15 21:57:00 INFO SparkHadoopMapRedUtil: attempt_202401152156542740283994940460430_0015_m_000002_0: Committed. Elapsed time: 9 ms.\n",
            "24/01/15 21:57:00 INFO Executor: Finished task 2.0 in stage 2.0 (TID 9). 2265 bytes result sent to driver\n",
            "24/01/15 21:57:00 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 9) in 386 ms on 03d6364d9dd2 (executor driver) (4/4)\n",
            "24/01/15 21:57:00 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "24/01/15 21:57:00 INFO DAGScheduler: ResultStage 2 (runJob at SparkHadoopWriter.scala:83) finished in 0.882 s\n",
            "24/01/15 21:57:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/01/15 21:57:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "24/01/15 21:57:00 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 5.655352 s\n",
            "24/01/15 21:57:00 INFO SparkHadoopWriter: Start to commit write Job job_202401152156542740283994940460430_0015.\n",
            "24/01/15 21:57:00 INFO SparkHadoopWriter: Write Job job_202401152156542740283994940460430_0015 committed. Elapsed time: 33 ms.\n",
            "24/01/15 21:57:00 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "24/01/15 21:57:00 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/01/15 21:57:00 INFO SparkUI: Stopped Spark web UI at http://03d6364d9dd2:4041\n",
            "24/01/15 21:57:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/01/15 21:57:00 INFO MemoryStore: MemoryStore cleared\n",
            "24/01/15 21:57:00 INFO BlockManager: BlockManager stopped\n",
            "24/01/15 21:57:00 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/01/15 21:57:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/01/15 21:57:01 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/01/15 21:57:01 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/01/15 21:57:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-4f832a39-9f70-41d1-8822-09f42ebf096f\n",
            "24/01/15 21:57:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-3ecdb1ea-175b-44ae-921e-0f9eedbc7771\n",
            "24/01/15 21:57:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-3ecdb1ea-175b-44ae-921e-0f9eedbc7771/pyspark-fa96a9b9-f322-47fd-bb29-6c96cd022166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat actividad2/punto1/data/output/part-* | sort > actividad2/punto1/data/output.txt"
      ],
      "metadata": {
        "id": "socVlJLsgy9G"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat actividad2/punto1/data/output.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb8vkqdbgqNJ",
        "outputId": "22a68c04-a1b1-4a03-dfc4-e3572d60e960"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice;250.0\n",
            "Bob;201.0\n",
            "Luis;0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y4V26QWugueU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punto 2"
      ],
      "metadata": {
        "id": "NqroH11vkJvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip punto2/data/0302.zip"
      ],
      "metadata": {
        "id": "CAlkuZeUkLL_"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"actividad2/punto2/data/output/output.txt\"\n",
        "!$SPARK_HOME/bin/spark-submit \\\n",
        "    ./actividad2/punto2/src/spark-job.py \\\n",
        "    ./actividad2/punto2/data/input \\\n",
        "    ./actividad2/punto2/data/output/output.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkdVoFcc-6Qr",
        "outputId": "65757ed2-1317-43f0-ab47-b75b7da58b8a"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/01/15 21:57:21 INFO SparkContext: Running Spark version 3.4.2\n",
            "24/01/15 21:57:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/01/15 21:57:21 INFO ResourceUtils: ==============================================================\n",
            "24/01/15 21:57:21 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/01/15 21:57:21 INFO ResourceUtils: ==============================================================\n",
            "24/01/15 21:57:21 INFO SparkContext: Submitted application: CategoriaDeVideosMenosVista\n",
            "24/01/15 21:57:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/01/15 21:57:21 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/01/15 21:57:21 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/01/15 21:57:21 INFO SecurityManager: Changing view acls to: root\n",
            "24/01/15 21:57:21 INFO SecurityManager: Changing modify acls to: root\n",
            "24/01/15 21:57:21 INFO SecurityManager: Changing view acls groups to: \n",
            "24/01/15 21:57:21 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/01/15 21:57:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/01/15 21:57:22 INFO Utils: Successfully started service 'sparkDriver' on port 34391.\n",
            "24/01/15 21:57:22 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/01/15 21:57:22 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/01/15 21:57:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/01/15 21:57:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/01/15 21:57:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/01/15 21:57:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6a03c9c0-851f-4667-b541-560958289e6c\n",
            "24/01/15 21:57:22 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "24/01/15 21:57:22 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/01/15 21:57:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/01/15 21:57:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "24/01/15 21:57:22 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "24/01/15 21:57:23 INFO Executor: Starting executor ID driver on host 03d6364d9dd2\n",
            "24/01/15 21:57:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/01/15 21:57:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43321.\n",
            "24/01/15 21:57:23 INFO NettyBlockTransferService: Server created on 03d6364d9dd2:43321\n",
            "24/01/15 21:57:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/01/15 21:57:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 03d6364d9dd2, 43321, None)\n",
            "24/01/15 21:57:23 INFO BlockManagerMasterEndpoint: Registering block manager 03d6364d9dd2:43321 with 366.3 MiB RAM, BlockManagerId(driver, 03d6364d9dd2, 43321, None)\n",
            "24/01/15 21:57:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 03d6364d9dd2, 43321, None)\n",
            "24/01/15 21:57:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 03d6364d9dd2, 43321, None)\n",
            "24/01/15 21:57:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 343.7 KiB, free 366.0 MiB)\n",
            "24/01/15 21:57:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)\n",
            "24/01/15 21:57:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 03d6364d9dd2:43321 (size: 32.6 KiB, free: 366.3 MiB)\n",
            "24/01/15 21:57:25 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0\n",
            "24/01/15 21:57:25 INFO FileInputFormat: Total input files to process : 1\n",
            "24/01/15 21:57:25 INFO FileInputFormat: Total input files to process : 1\n",
            "24/01/15 21:57:25 INFO SparkContext: Starting job: reduce at /content/actividad2/punto2/src/spark-job.py:28\n",
            "24/01/15 21:57:25 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/actividad2/punto2/src/spark-job.py:25) as input to shuffle 0\n",
            "24/01/15 21:57:25 INFO DAGScheduler: Got job 0 (reduce at /content/actividad2/punto2/src/spark-job.py:28) with 1 output partitions\n",
            "24/01/15 21:57:25 INFO DAGScheduler: Final stage: ResultStage 1 (reduce at /content/actividad2/punto2/src/spark-job.py:28)\n",
            "24/01/15 21:57:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "24/01/15 21:57:25 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "24/01/15 21:57:25 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/actividad2/punto2/src/spark-job.py:25), which has no missing parents\n",
            "24/01/15 21:57:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.1 KiB, free 365.9 MiB)\n",
            "24/01/15 21:57:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 365.9 MiB)\n",
            "24/01/15 21:57:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 03d6364d9dd2:43321 (size: 7.8 KiB, free: 366.3 MiB)\n",
            "24/01/15 21:57:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "24/01/15 21:57:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/actividad2/punto2/src/spark-job.py:25) (first 15 tasks are for partitions Vector(0))\n",
            "24/01/15 21:57:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "24/01/15 21:57:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (03d6364d9dd2, executor driver, partition 0, PROCESS_LOCAL, 7492 bytes) \n",
            "24/01/15 21:57:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "24/01/15 21:57:26 INFO WholeTextFileRDD: Input split: Paths:/content/actividad2/punto2/data/input/0.txt:0+294\n",
            "24/01/15 21:57:28 INFO PythonRunner: Times: total = 1600, boot = 1334, init = 264, finish = 2\n",
            "24/01/15 21:57:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1623 bytes result sent to driver\n",
            "24/01/15 21:57:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2914 ms on 03d6364d9dd2 (executor driver) (1/1)\n",
            "24/01/15 21:57:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/01/15 21:57:29 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 58833\n",
            "24/01/15 21:57:29 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/actividad2/punto2/src/spark-job.py:25) finished in 3.121 s\n",
            "24/01/15 21:57:29 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/01/15 21:57:29 INFO DAGScheduler: running: Set()\n",
            "24/01/15 21:57:29 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "24/01/15 21:57:29 INFO DAGScheduler: failed: Set()\n",
            "24/01/15 21:57:29 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at reduce at /content/actividad2/punto2/src/spark-job.py:28), which has no missing parents\n",
            "24/01/15 21:57:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.5 KiB, free 365.9 MiB)\n",
            "24/01/15 21:57:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.9 MiB)\n",
            "24/01/15 21:57:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 03d6364d9dd2:43321 (size: 6.2 KiB, free: 366.3 MiB)\n",
            "24/01/15 21:57:29 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "24/01/15 21:57:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[6] at reduce at /content/actividad2/punto2/src/spark-job.py:28) (first 15 tasks are for partitions Vector(0))\n",
            "24/01/15 21:57:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "24/01/15 21:57:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (03d6364d9dd2, executor driver, partition 0, NODE_LOCAL, 7181 bytes) \n",
            "24/01/15 21:57:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "24/01/15 21:57:29 INFO ShuffleBlockFetcherIterator: Getting 1 (117.0 B) non-empty blocks including 1 (117.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 45 ms\n",
            "24/01/15 21:57:29 INFO PythonRunner: Times: total = 149, boot = -965, init = 1114, finish = 0\n",
            "24/01/15 21:57:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2065 bytes result sent to driver\n",
            "24/01/15 21:57:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 391 ms on 03d6364d9dd2 (executor driver) (1/1)\n",
            "24/01/15 21:57:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "24/01/15 21:57:29 INFO DAGScheduler: ResultStage 1 (reduce at /content/actividad2/punto2/src/spark-job.py:28) finished in 0.431 s\n",
            "24/01/15 21:57:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/01/15 21:57:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "24/01/15 21:57:29 INFO DAGScheduler: Job 0 finished: reduce at /content/actividad2/punto2/src/spark-job.py:28, took 3.946050 s\n",
            "24/01/15 21:57:29 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "24/01/15 21:57:29 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/01/15 21:57:29 INFO SparkUI: Stopped Spark web UI at http://03d6364d9dd2:4041\n",
            "24/01/15 21:57:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/01/15 21:57:29 INFO MemoryStore: MemoryStore cleared\n",
            "24/01/15 21:57:29 INFO BlockManager: BlockManager stopped\n",
            "24/01/15 21:57:29 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/01/15 21:57:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/01/15 21:57:29 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/01/15 21:57:29 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/01/15 21:57:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-a97c0708-d6b0-464b-8389-af3f504e86a1/pyspark-b51b8b07-6c32-49a8-97d2-69ce74e5baad\n",
            "24/01/15 21:57:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-a97c0708-d6b0-464b-8389-af3f504e86a1\n",
            "24/01/15 21:57:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-48e0d641-8343-4d82-952f-56b749066a11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat actividad2/punto2/data/output/output.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeGJqHmIIapT",
        "outputId": "550de5ef-3000-418d-c9b9-3937147085c8"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sports;20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"actividad2/punto2/data/output/0302_output.txt\"\n",
        "!$SPARK_HOME/bin/spark-submit \\\n",
        "    ./actividad2/punto2/src/spark-job.py \\\n",
        "    ./actividad2/punto2/data/0302 \\\n",
        "    ./actividad2/punto2/data/output/0302_output.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuyKCOD3kLJo",
        "outputId": "2fff6c47-0039-4e69-a125-9506e1965b7f"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/01/15 21:57:42 INFO SparkContext: Running Spark version 3.4.2\n",
            "24/01/15 21:57:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/01/15 21:57:42 INFO ResourceUtils: ==============================================================\n",
            "24/01/15 21:57:42 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/01/15 21:57:42 INFO ResourceUtils: ==============================================================\n",
            "24/01/15 21:57:42 INFO SparkContext: Submitted application: CategoriaDeVideosMenosVista\n",
            "24/01/15 21:57:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/01/15 21:57:42 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/01/15 21:57:42 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/01/15 21:57:42 INFO SecurityManager: Changing view acls to: root\n",
            "24/01/15 21:57:42 INFO SecurityManager: Changing modify acls to: root\n",
            "24/01/15 21:57:42 INFO SecurityManager: Changing view acls groups to: \n",
            "24/01/15 21:57:42 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/01/15 21:57:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/01/15 21:57:42 INFO Utils: Successfully started service 'sparkDriver' on port 39669.\n",
            "24/01/15 21:57:42 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/01/15 21:57:43 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/01/15 21:57:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/01/15 21:57:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/01/15 21:57:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/01/15 21:57:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a296f9d9-2a17-4444-8105-40d5e7668dfe\n",
            "24/01/15 21:57:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "24/01/15 21:57:43 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/01/15 21:57:43 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/01/15 21:57:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "24/01/15 21:57:44 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "24/01/15 21:57:44 INFO Executor: Starting executor ID driver on host 03d6364d9dd2\n",
            "24/01/15 21:57:44 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/01/15 21:57:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40671.\n",
            "24/01/15 21:57:44 INFO NettyBlockTransferService: Server created on 03d6364d9dd2:40671\n",
            "24/01/15 21:57:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/01/15 21:57:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 03d6364d9dd2, 40671, None)\n",
            "24/01/15 21:57:44 INFO BlockManagerMasterEndpoint: Registering block manager 03d6364d9dd2:40671 with 366.3 MiB RAM, BlockManagerId(driver, 03d6364d9dd2, 40671, None)\n",
            "24/01/15 21:57:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 03d6364d9dd2, 40671, None)\n",
            "24/01/15 21:57:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 03d6364d9dd2, 40671, None)\n",
            "24/01/15 21:57:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 343.7 KiB, free 366.0 MiB)\n",
            "24/01/15 21:57:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)\n",
            "24/01/15 21:57:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 03d6364d9dd2:40671 (size: 32.6 KiB, free: 366.3 MiB)\n",
            "24/01/15 21:57:47 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0\n",
            "24/01/15 21:57:47 INFO FileInputFormat: Total input files to process : 4\n",
            "24/01/15 21:57:47 INFO FileInputFormat: Total input files to process : 4\n",
            "24/01/15 21:57:48 INFO SparkContext: Starting job: reduce at /content/actividad2/punto2/src/spark-job.py:28\n",
            "24/01/15 21:57:48 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/actividad2/punto2/src/spark-job.py:25) as input to shuffle 0\n",
            "24/01/15 21:57:48 INFO DAGScheduler: Got job 0 (reduce at /content/actividad2/punto2/src/spark-job.py:28) with 2 output partitions\n",
            "24/01/15 21:57:48 INFO DAGScheduler: Final stage: ResultStage 1 (reduce at /content/actividad2/punto2/src/spark-job.py:28)\n",
            "24/01/15 21:57:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "24/01/15 21:57:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "24/01/15 21:57:48 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/actividad2/punto2/src/spark-job.py:25), which has no missing parents\n",
            "24/01/15 21:57:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.1 KiB, free 365.9 MiB)\n",
            "24/01/15 21:57:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 365.9 MiB)\n",
            "24/01/15 21:57:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 03d6364d9dd2:40671 (size: 7.8 KiB, free: 366.3 MiB)\n",
            "24/01/15 21:57:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "24/01/15 21:57:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/actividad2/punto2/src/spark-job.py:25) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/01/15 21:57:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "24/01/15 21:57:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (03d6364d9dd2, executor driver, partition 0, PROCESS_LOCAL, 7622 bytes) \n",
            "24/01/15 21:57:48 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (03d6364d9dd2, executor driver, partition 1, PROCESS_LOCAL, 7493 bytes) \n",
            "24/01/15 21:57:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "24/01/15 21:57:48 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "24/01/15 21:57:49 INFO WholeTextFileRDD: Input split: Paths:/content/actividad2/punto2/data/0302/0.txt:0+59091,/content/actividad2/punto2/data/0302/1.txt:0+456948,/content/actividad2/punto2/data/0302/2.txt:0+2438282\n",
            "24/01/15 21:57:49 INFO WholeTextFileRDD: Input split: Paths:/content/actividad2/punto2/data/0302/log.txt:0+150\n",
            "24/01/15 21:57:50 INFO PythonRunner: Times: total = 1446, boot = 1153, init = 292, finish = 1\n",
            "24/01/15 21:57:50 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1495 bytes result sent to driver\n",
            "24/01/15 21:57:50 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2374 ms on 03d6364d9dd2 (executor driver) (1/2)\n",
            "24/01/15 21:57:51 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 42177\n",
            "24/01/15 21:57:51 INFO PythonRunner: Times: total = 1495, boot = 1169, init = 253, finish = 73\n",
            "24/01/15 21:57:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1581 bytes result sent to driver\n",
            "24/01/15 21:57:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2929 ms on 03d6364d9dd2 (executor driver) (2/2)\n",
            "24/01/15 21:57:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/01/15 21:57:51 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/actividad2/punto2/src/spark-job.py:25) finished in 3.166 s\n",
            "24/01/15 21:57:51 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/01/15 21:57:51 INFO DAGScheduler: running: Set()\n",
            "24/01/15 21:57:51 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "24/01/15 21:57:51 INFO DAGScheduler: failed: Set()\n",
            "24/01/15 21:57:51 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at reduce at /content/actividad2/punto2/src/spark-job.py:28), which has no missing parents\n",
            "24/01/15 21:57:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.5 KiB, free 365.9 MiB)\n",
            "24/01/15 21:57:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.9 MiB)\n",
            "24/01/15 21:57:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 03d6364d9dd2:40671 (size: 6.2 KiB, free: 366.3 MiB)\n",
            "24/01/15 21:57:51 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "24/01/15 21:57:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at reduce at /content/actividad2/punto2/src/spark-job.py:28) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/01/15 21:57:51 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "24/01/15 21:57:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (03d6364d9dd2, executor driver, partition 0, NODE_LOCAL, 7181 bytes) \n",
            "24/01/15 21:57:51 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (03d6364d9dd2, executor driver, partition 1, NODE_LOCAL, 7181 bytes) \n",
            "24/01/15 21:57:51 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
            "24/01/15 21:57:51 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\n",
            "24/01/15 21:57:51 INFO ShuffleBlockFetcherIterator: Getting 1 (276.0 B) non-empty blocks including 1 (276.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:57:51 INFO ShuffleBlockFetcherIterator: Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:57:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 27 ms\n",
            "24/01/15 21:57:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 29 ms\n",
            "24/01/15 21:57:51 INFO PythonRunner: Times: total = 188, boot = -786, init = 974, finish = 0\n",
            "24/01/15 21:57:51 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 2077 bytes result sent to driver\n",
            "24/01/15 21:57:51 INFO PythonRunner: Times: total = 206, boot = -778, init = 984, finish = 0\n",
            "24/01/15 21:57:51 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 2077 bytes result sent to driver\n",
            "24/01/15 21:57:51 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 354 ms on 03d6364d9dd2 (executor driver) (1/2)\n",
            "24/01/15 21:57:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 362 ms on 03d6364d9dd2 (executor driver) (2/2)\n",
            "24/01/15 21:57:51 INFO DAGScheduler: ResultStage 1 (reduce at /content/actividad2/punto2/src/spark-job.py:28) finished in 0.405 s\n",
            "24/01/15 21:57:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "24/01/15 21:57:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/01/15 21:57:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "24/01/15 21:57:51 INFO DAGScheduler: Job 0 finished: reduce at /content/actividad2/punto2/src/spark-job.py:28, took 3.925898 s\n",
            "24/01/15 21:57:52 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "24/01/15 21:57:52 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/01/15 21:57:52 INFO SparkUI: Stopped Spark web UI at http://03d6364d9dd2:4041\n",
            "24/01/15 21:57:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/01/15 21:57:52 INFO MemoryStore: MemoryStore cleared\n",
            "24/01/15 21:57:52 INFO BlockManager: BlockManager stopped\n",
            "24/01/15 21:57:52 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/01/15 21:57:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/01/15 21:57:52 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/01/15 21:57:52 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/01/15 21:57:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-ac0de0a0-7e8f-4ff3-891e-644d07f0bb38/pyspark-6182003f-dba2-416b-b5bb-24bff6782e7c\n",
            "24/01/15 21:57:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4e52822-9f82-4afa-a845-ef1c929d0677\n",
            "24/01/15 21:57:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-ac0de0a0-7e8f-4ff3-891e-644d07f0bb38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat actividad2/punto2/data/output/0302_output.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X97eSi1JkLHe",
        "outputId": "8b7d4206-a0c8-46dd-c343-1991bccd4b9e"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Travel & Places;305409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-SY2VdhNkLFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punto 3"
      ],
      "metadata": {
        "id": "5MEqDgvMkJrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"actividad2/punto3/data/output\"\n",
        "!$SPARK_HOME/bin/spark-submit \\\n",
        "    ./actividad2/punto3/src/spark-job.py \\\n",
        "    ./actividad2/punto3/data/input.txt \\\n",
        "    ./actividad2/punto3/data/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le-wKIdekKXO",
        "outputId": "9de864a8-a207-43db-e23b-46fc3278e0dd"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/01/15 21:57:59 INFO SparkContext: Running Spark version 3.4.2\n",
            "24/01/15 21:57:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/01/15 21:57:59 INFO ResourceUtils: ==============================================================\n",
            "24/01/15 21:57:59 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/01/15 21:57:59 INFO ResourceUtils: ==============================================================\n",
            "24/01/15 21:57:59 INFO SparkContext: Submitted application: personaYMetodosDePago\n",
            "24/01/15 21:57:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/01/15 21:57:59 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/01/15 21:57:59 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/01/15 21:57:59 INFO SecurityManager: Changing view acls to: root\n",
            "24/01/15 21:57:59 INFO SecurityManager: Changing modify acls to: root\n",
            "24/01/15 21:57:59 INFO SecurityManager: Changing view acls groups to: \n",
            "24/01/15 21:57:59 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/01/15 21:57:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/01/15 21:58:00 INFO Utils: Successfully started service 'sparkDriver' on port 42177.\n",
            "24/01/15 21:58:01 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/01/15 21:58:01 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/01/15 21:58:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/01/15 21:58:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/01/15 21:58:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/01/15 21:58:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c60cb64f-d0ad-41f1-a9ce-dd2e6fc73be2\n",
            "24/01/15 21:58:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "24/01/15 21:58:01 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/01/15 21:58:01 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/01/15 21:58:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "24/01/15 21:58:01 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "24/01/15 21:58:02 INFO Executor: Starting executor ID driver on host 03d6364d9dd2\n",
            "24/01/15 21:58:02 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/01/15 21:58:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42071.\n",
            "24/01/15 21:58:02 INFO NettyBlockTransferService: Server created on 03d6364d9dd2:42071\n",
            "24/01/15 21:58:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/01/15 21:58:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 03d6364d9dd2, 42071, None)\n",
            "24/01/15 21:58:02 INFO BlockManagerMasterEndpoint: Registering block manager 03d6364d9dd2:42071 with 366.3 MiB RAM, BlockManagerId(driver, 03d6364d9dd2, 42071, None)\n",
            "24/01/15 21:58:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 03d6364d9dd2, 42071, None)\n",
            "24/01/15 21:58:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 03d6364d9dd2, 42071, None)\n",
            "24/01/15 21:58:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 358.0 KiB, free 366.0 MiB)\n",
            "24/01/15 21:58:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.9 MiB)\n",
            "24/01/15 21:58:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 03d6364d9dd2:42071 (size: 32.5 KiB, free: 366.3 MiB)\n",
            "24/01/15 21:58:04 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "24/01/15 21:58:04 INFO FileInputFormat: Total input files to process : 1\n",
            "24/01/15 21:58:04 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "24/01/15 21:58:04 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:58:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:58:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:58:04 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "24/01/15 21:58:05 INFO DAGScheduler: Registering RDD 3 (distinct at /content/actividad2/punto3/src/spark-job.py:21) as input to shuffle 1\n",
            "24/01/15 21:58:05 INFO DAGScheduler: Registering RDD 10 (reduceByKey at /content/actividad2/punto3/src/spark-job.py:34) as input to shuffle 0\n",
            "24/01/15 21:58:05 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 4 output partitions\n",
            "24/01/15 21:58:05 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at SparkHadoopWriter.scala:83)\n",
            "24/01/15 21:58:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
            "24/01/15 21:58:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
            "24/01/15 21:58:05 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at distinct at /content/actividad2/punto3/src/spark-job.py:21), which has no missing parents\n",
            "24/01/15 21:58:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)\n",
            "24/01/15 21:58:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 365.9 MiB)\n",
            "24/01/15 21:58:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 03d6364d9dd2:42071 (size: 7.5 KiB, free: 366.3 MiB)\n",
            "24/01/15 21:58:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "24/01/15 21:58:05 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at distinct at /content/actividad2/punto3/src/spark-job.py:21) (first 15 tasks are for partitions Vector(0, 1))\n",
            "24/01/15 21:58:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "24/01/15 21:58:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (03d6364d9dd2, executor driver, partition 0, PROCESS_LOCAL, 7414 bytes) \n",
            "24/01/15 21:58:05 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (03d6364d9dd2, executor driver, partition 1, PROCESS_LOCAL, 7414 bytes) \n",
            "24/01/15 21:58:05 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "24/01/15 21:58:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "24/01/15 21:58:06 INFO HadoopRDD: Input split: file:/content/actividad2/punto3/data/input.txt:76+77\n",
            "24/01/15 21:58:06 INFO HadoopRDD: Input split: file:/content/actividad2/punto3/data/input.txt:0+76\n",
            "24/01/15 21:58:07 INFO PythonRunner: Times: total = 1086, boot = 836, init = 248, finish = 2\n",
            "24/01/15 21:58:07 INFO PythonRunner: Times: total = 1095, boot = 820, init = 273, finish = 2\n",
            "24/01/15 21:58:07 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1624 bytes result sent to driver\n",
            "24/01/15 21:58:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1624 bytes result sent to driver\n",
            "24/01/15 21:58:07 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2380 ms on 03d6364d9dd2 (executor driver) (1/2)\n",
            "24/01/15 21:58:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2432 ms on 03d6364d9dd2 (executor driver) (2/2)\n",
            "24/01/15 21:58:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/01/15 21:58:07 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50465\n",
            "24/01/15 21:58:07 INFO DAGScheduler: ShuffleMapStage 0 (distinct at /content/actividad2/punto3/src/spark-job.py:21) finished in 2.710 s\n",
            "24/01/15 21:58:07 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/01/15 21:58:07 INFO DAGScheduler: running: Set()\n",
            "24/01/15 21:58:07 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)\n",
            "24/01/15 21:58:07 INFO DAGScheduler: failed: Set()\n",
            "24/01/15 21:58:07 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[10] at reduceByKey at /content/actividad2/punto3/src/spark-job.py:34), which has no missing parents\n",
            "24/01/15 21:58:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 19.1 KiB, free 365.9 MiB)\n",
            "24/01/15 21:58:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.9 MiB)\n",
            "24/01/15 21:58:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 03d6364d9dd2:42071 (size: 9.0 KiB, free: 366.3 MiB)\n",
            "24/01/15 21:58:08 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
            "24/01/15 21:58:08 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 1 (PairwiseRDD[10] at reduceByKey at /content/actividad2/punto3/src/spark-job.py:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "24/01/15 21:58:08 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0\n",
            "24/01/15 21:58:08 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 2) (03d6364d9dd2, executor driver, partition 3, NODE_LOCAL, 7279 bytes) \n",
            "24/01/15 21:58:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3) (03d6364d9dd2, executor driver, partition 0, PROCESS_LOCAL, 7523 bytes) \n",
            "24/01/15 21:58:08 INFO Executor: Running task 3.0 in stage 1.0 (TID 2)\n",
            "24/01/15 21:58:08 INFO Executor: Running task 0.0 in stage 1.0 (TID 3)\n",
            "24/01/15 21:58:08 INFO HadoopRDD: Input split: file:/content/actividad2/punto3/data/input.txt:0+76\n",
            "24/01/15 21:58:08 INFO ShuffleBlockFetcherIterator: Getting 2 (168.0 B) non-empty blocks including 2 (168.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:58:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 50 ms\n",
            "24/01/15 21:58:08 INFO PythonRunner: Times: total = 342, boot = -733, init = 1075, finish = 0\n",
            "24/01/15 21:58:08 INFO PythonRunner: Times: total = 383, boot = -765, init = 1148, finish = 0\n",
            "24/01/15 21:58:08 INFO Executor: Finished task 0.0 in stage 1.0 (TID 3). 1454 bytes result sent to driver\n",
            "24/01/15 21:58:08 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4) (03d6364d9dd2, executor driver, partition 1, PROCESS_LOCAL, 7523 bytes) \n",
            "24/01/15 21:58:08 INFO Executor: Running task 1.0 in stage 1.0 (TID 4)\n",
            "24/01/15 21:58:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 502 ms on 03d6364d9dd2 (executor driver) (1/4)\n",
            "24/01/15 21:58:08 INFO HadoopRDD: Input split: file:/content/actividad2/punto3/data/input.txt:76+77\n",
            "24/01/15 21:58:08 INFO PythonRunner: Times: total = 448, boot = 30, init = 413, finish = 5\n",
            "24/01/15 21:58:08 INFO PythonRunner: Times: total = 505, boot = 31, init = 472, finish = 2\n",
            "24/01/15 21:58:08 INFO Executor: Finished task 3.0 in stage 1.0 (TID 2). 2228 bytes result sent to driver\n",
            "24/01/15 21:58:08 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5) (03d6364d9dd2, executor driver, partition 2, PROCESS_LOCAL, 7279 bytes) \n",
            "24/01/15 21:58:08 INFO Executor: Running task 2.0 in stage 1.0 (TID 5)\n",
            "24/01/15 21:58:08 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 2) in 872 ms on 03d6364d9dd2 (executor driver) (2/4)\n",
            "24/01/15 21:58:08 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:58:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/01/15 21:58:08 INFO PythonRunner: Times: total = 427, boot = -68, init = 495, finish = 0\n",
            "24/01/15 21:58:09 INFO PythonRunner: Times: total = 389, boot = -2, init = 391, finish = 0\n",
            "24/01/15 21:58:09 INFO Executor: Finished task 1.0 in stage 1.0 (TID 4). 1583 bytes result sent to driver\n",
            "24/01/15 21:58:09 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 581 ms on 03d6364d9dd2 (executor driver) (3/4)\n",
            "24/01/15 21:58:09 INFO PythonRunner: Times: total = 300, boot = -182, init = 482, finish = 0\n",
            "24/01/15 21:58:09 INFO PythonRunner: Times: total = 281, boot = -31, init = 312, finish = 0\n",
            "24/01/15 21:58:09 INFO Executor: Finished task 2.0 in stage 1.0 (TID 5). 2099 bytes result sent to driver\n",
            "24/01/15 21:58:09 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 376 ms on 03d6364d9dd2 (executor driver) (4/4)\n",
            "24/01/15 21:58:09 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "24/01/15 21:58:09 INFO DAGScheduler: ShuffleMapStage 1 (reduceByKey at /content/actividad2/punto3/src/spark-job.py:34) finished in 1.273 s\n",
            "24/01/15 21:58:09 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/01/15 21:58:09 INFO DAGScheduler: running: Set()\n",
            "24/01/15 21:58:09 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
            "24/01/15 21:58:09 INFO DAGScheduler: failed: Set()\n",
            "24/01/15 21:58:09 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[21] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "24/01/15 21:58:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 108.6 KiB, free 365.8 MiB)\n",
            "24/01/15 21:58:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 41.2 KiB, free 365.7 MiB)\n",
            "24/01/15 21:58:09 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 03d6364d9dd2:42071 (size: 41.2 KiB, free: 366.2 MiB)\n",
            "24/01/15 21:58:09 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
            "24/01/15 21:58:09 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 2 (MapPartitionsRDD[21] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "24/01/15 21:58:09 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks resource profile 0\n",
            "24/01/15 21:58:09 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6) (03d6364d9dd2, executor driver, partition 1, NODE_LOCAL, 7181 bytes) \n",
            "24/01/15 21:58:09 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 7) (03d6364d9dd2, executor driver, partition 3, NODE_LOCAL, 7181 bytes) \n",
            "24/01/15 21:58:09 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)\n",
            "24/01/15 21:58:09 INFO Executor: Running task 3.0 in stage 2.0 (TID 7)\n",
            "24/01/15 21:58:09 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/01/15 21:58:09 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:58:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:58:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:58:09 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n",
            "24/01/15 21:58:09 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:58:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:58:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:58:09 INFO PythonRunner: Times: total = 244, boot = -414, init = 658, finish = 0\n",
            "24/01/15 21:58:09 INFO FileOutputCommitter: Saved output of task 'attempt_202401152158044348915752942920055_0021_m_000001_0' to file:/content/actividad2/punto3/data/output/comprasSinTDCMayorDe1500/_temporary/0/task_202401152158044348915752942920055_0021_m_000001\n",
            "24/01/15 21:58:09 INFO SparkHadoopMapRedUtil: attempt_202401152158044348915752942920055_0021_m_000001_0: Committed. Elapsed time: 4 ms.\n",
            "24/01/15 21:58:09 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 2394 bytes result sent to driver\n",
            "24/01/15 21:58:09 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8) (03d6364d9dd2, executor driver, partition 0, PROCESS_LOCAL, 7181 bytes) \n",
            "24/01/15 21:58:09 INFO Executor: Running task 0.0 in stage 2.0 (TID 8)\n",
            "24/01/15 21:58:09 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 418 ms on 03d6364d9dd2 (executor driver) (1/4)\n",
            "24/01/15 21:58:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/01/15 21:58:09 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:58:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:58:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:58:09 INFO PythonRunner: Times: total = 357, boot = -373, init = 729, finish = 1\n",
            "24/01/15 21:58:09 INFO FileOutputCommitter: Saved output of task 'attempt_202401152158044348915752942920055_0021_m_000003_0' to file:/content/actividad2/punto3/data/output/comprasSinTDCMayorDe1500/_temporary/0/task_202401152158044348915752942920055_0021_m_000003\n",
            "24/01/15 21:58:09 INFO SparkHadoopMapRedUtil: attempt_202401152158044348915752942920055_0021_m_000003_0: Committed. Elapsed time: 11 ms.\n",
            "24/01/15 21:58:09 INFO Executor: Finished task 3.0 in stage 2.0 (TID 7). 2394 bytes result sent to driver\n",
            "24/01/15 21:58:09 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 9) (03d6364d9dd2, executor driver, partition 2, PROCESS_LOCAL, 7181 bytes) \n",
            "24/01/15 21:58:09 INFO Executor: Running task 2.0 in stage 2.0 (TID 9)\n",
            "24/01/15 21:58:09 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 7) in 540 ms on 03d6364d9dd2 (executor driver) (2/4)\n",
            "24/01/15 21:58:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/01/15 21:58:10 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:58:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:58:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:58:10 INFO PythonRunner: Times: total = 263, boot = -495, init = 758, finish = 0\n",
            "24/01/15 21:58:10 INFO FileOutputCommitter: Saved output of task 'attempt_202401152158044348915752942920055_0021_m_000000_0' to file:/content/actividad2/punto3/data/output/comprasSinTDCMayorDe1500/_temporary/0/task_202401152158044348915752942920055_0021_m_000000\n",
            "24/01/15 21:58:10 INFO SparkHadoopMapRedUtil: attempt_202401152158044348915752942920055_0021_m_000000_0: Committed. Elapsed time: 1 ms.\n",
            "24/01/15 21:58:10 INFO Executor: Finished task 0.0 in stage 2.0 (TID 8). 2308 bytes result sent to driver\n",
            "24/01/15 21:58:10 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 337 ms on 03d6364d9dd2 (executor driver) (3/4)\n",
            "24/01/15 21:58:10 INFO PythonRunner: Times: total = 195, boot = -662, init = 856, finish = 1\n",
            "24/01/15 21:58:10 INFO FileOutputCommitter: Saved output of task 'attempt_202401152158044348915752942920055_0021_m_000002_0' to file:/content/actividad2/punto3/data/output/comprasSinTDCMayorDe1500/_temporary/0/task_202401152158044348915752942920055_0021_m_000002\n",
            "24/01/15 21:58:10 INFO SparkHadoopMapRedUtil: attempt_202401152158044348915752942920055_0021_m_000002_0: Committed. Elapsed time: 1 ms.\n",
            "24/01/15 21:58:10 INFO Executor: Finished task 2.0 in stage 2.0 (TID 9). 2351 bytes result sent to driver\n",
            "24/01/15 21:58:10 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 9) in 375 ms on 03d6364d9dd2 (executor driver) (4/4)\n",
            "24/01/15 21:58:10 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "24/01/15 21:58:10 INFO DAGScheduler: ResultStage 2 (runJob at SparkHadoopWriter.scala:83) finished in 0.977 s\n",
            "24/01/15 21:58:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/01/15 21:58:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "24/01/15 21:58:10 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 5.450180 s\n",
            "24/01/15 21:58:10 INFO SparkHadoopWriter: Start to commit write Job job_202401152158044348915752942920055_0021.\n",
            "24/01/15 21:58:10 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 03d6364d9dd2:42071 in memory (size: 7.5 KiB, free: 366.2 MiB)\n",
            "24/01/15 21:58:10 INFO SparkHadoopWriter: Write Job job_202401152158044348915752942920055_0021 committed. Elapsed time: 53 ms.\n",
            "24/01/15 21:58:10 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 03d6364d9dd2:42071 in memory (size: 9.0 KiB, free: 366.2 MiB)\n",
            "24/01/15 21:58:10 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:58:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:58:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:58:10 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "24/01/15 21:58:10 INFO DAGScheduler: Registering RDD 16 (reduceByKey at /content/actividad2/punto3/src/spark-job.py:41) as input to shuffle 2\n",
            "24/01/15 21:58:10 INFO DAGScheduler: Got job 1 (runJob at SparkHadoopWriter.scala:83) with 4 output partitions\n",
            "24/01/15 21:58:10 INFO DAGScheduler: Final stage: ResultStage 5 (runJob at SparkHadoopWriter.scala:83)\n",
            "24/01/15 21:58:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "24/01/15 21:58:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)\n",
            "24/01/15 21:58:10 INFO DAGScheduler: Submitting ShuffleMapStage 4 (PairwiseRDD[16] at reduceByKey at /content/actividad2/punto3/src/spark-job.py:41), which has no missing parents\n",
            "24/01/15 21:58:10 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 19.1 KiB, free 365.8 MiB)\n",
            "24/01/15 21:58:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.7 MiB)\n",
            "24/01/15 21:58:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 03d6364d9dd2:42071 (size: 9.0 KiB, free: 366.2 MiB)\n",
            "24/01/15 21:58:10 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535\n",
            "24/01/15 21:58:10 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 4 (PairwiseRDD[16] at reduceByKey at /content/actividad2/punto3/src/spark-job.py:41) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "24/01/15 21:58:10 INFO TaskSchedulerImpl: Adding task set 4.0 with 4 tasks resource profile 0\n",
            "24/01/15 21:58:10 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 10) (03d6364d9dd2, executor driver, partition 3, NODE_LOCAL, 7279 bytes) \n",
            "24/01/15 21:58:10 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 11) (03d6364d9dd2, executor driver, partition 0, PROCESS_LOCAL, 7523 bytes) \n",
            "24/01/15 21:58:10 INFO Executor: Running task 3.0 in stage 4.0 (TID 10)\n",
            "24/01/15 21:58:10 INFO Executor: Running task 0.0 in stage 4.0 (TID 11)\n",
            "24/01/15 21:58:10 INFO HadoopRDD: Input split: file:/content/actividad2/punto3/data/input.txt:0+76\n",
            "24/01/15 21:58:10 INFO ShuffleBlockFetcherIterator: Getting 2 (168.0 B) non-empty blocks including 2 (168.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 63 ms\n",
            "24/01/15 21:58:10 INFO PythonRunner: Times: total = 441, boot = -437, init = 878, finish = 0\n",
            "24/01/15 21:58:11 INFO PythonRunner: Times: total = 520, boot = -728, init = 1248, finish = 0\n",
            "24/01/15 21:58:11 INFO PythonRunner: Times: total = 514, boot = -600, init = 1114, finish = 0\n",
            "24/01/15 21:58:11 INFO Executor: Finished task 0.0 in stage 4.0 (TID 11). 1454 bytes result sent to driver\n",
            "24/01/15 21:58:11 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 12) (03d6364d9dd2, executor driver, partition 1, PROCESS_LOCAL, 7523 bytes) \n",
            "24/01/15 21:58:11 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 11) in 580 ms on 03d6364d9dd2 (executor driver) (1/4)\n",
            "24/01/15 21:58:11 INFO Executor: Running task 1.0 in stage 4.0 (TID 12)\n",
            "24/01/15 21:58:11 INFO PythonRunner: Times: total = 472, boot = -389, init = 860, finish = 1\n",
            "24/01/15 21:58:11 INFO HadoopRDD: Input split: file:/content/actividad2/punto3/data/input.txt:76+77\n",
            "24/01/15 21:58:11 INFO Executor: Finished task 3.0 in stage 4.0 (TID 10). 2228 bytes result sent to driver\n",
            "24/01/15 21:58:11 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 13) (03d6364d9dd2, executor driver, partition 2, PROCESS_LOCAL, 7279 bytes) \n",
            "24/01/15 21:58:11 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 10) in 643 ms on 03d6364d9dd2 (executor driver) (2/4)\n",
            "24/01/15 21:58:11 INFO Executor: Running task 2.0 in stage 4.0 (TID 13)\n",
            "24/01/15 21:58:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:58:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
            "24/01/15 21:58:11 INFO PythonRunner: Times: total = 592, boot = -73, init = 665, finish = 0\n",
            "24/01/15 21:58:11 INFO PythonRunner: Times: total = 576, boot = -37, init = 613, finish = 0\n",
            "24/01/15 21:58:11 INFO PythonRunner: Times: total = 571, boot = -63, init = 634, finish = 0\n",
            "24/01/15 21:58:11 INFO Executor: Finished task 1.0 in stage 4.0 (TID 12). 1583 bytes result sent to driver\n",
            "24/01/15 21:58:11 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 12) in 727 ms on 03d6364d9dd2 (executor driver) (3/4)\n",
            "24/01/15 21:58:11 INFO PythonRunner: Times: total = 593, boot = -126, init = 719, finish = 0\n",
            "24/01/15 21:58:11 INFO Executor: Finished task 2.0 in stage 4.0 (TID 13). 2142 bytes result sent to driver\n",
            "24/01/15 21:58:11 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 13) in 733 ms on 03d6364d9dd2 (executor driver) (4/4)\n",
            "24/01/15 21:58:11 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "24/01/15 21:58:11 INFO DAGScheduler: ShuffleMapStage 4 (reduceByKey at /content/actividad2/punto3/src/spark-job.py:41) finished in 1.417 s\n",
            "24/01/15 21:58:11 INFO DAGScheduler: looking for newly runnable stages\n",
            "24/01/15 21:58:11 INFO DAGScheduler: running: Set()\n",
            "24/01/15 21:58:11 INFO DAGScheduler: waiting: Set(ResultStage 5)\n",
            "24/01/15 21:58:11 INFO DAGScheduler: failed: Set()\n",
            "24/01/15 21:58:11 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[24] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "24/01/15 21:58:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 108.6 KiB, free 365.6 MiB)\n",
            "24/01/15 21:58:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 41.2 KiB, free 365.6 MiB)\n",
            "24/01/15 21:58:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 03d6364d9dd2:42071 (size: 41.2 KiB, free: 366.2 MiB)\n",
            "24/01/15 21:58:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
            "24/01/15 21:58:11 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[24] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "24/01/15 21:58:11 INFO TaskSchedulerImpl: Adding task set 5.0 with 4 tasks resource profile 0\n",
            "24/01/15 21:58:11 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 14) (03d6364d9dd2, executor driver, partition 1, NODE_LOCAL, 7181 bytes) \n",
            "24/01/15 21:58:11 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 15) (03d6364d9dd2, executor driver, partition 3, NODE_LOCAL, 7181 bytes) \n",
            "24/01/15 21:58:11 INFO Executor: Running task 1.0 in stage 5.0 (TID 14)\n",
            "24/01/15 21:58:11 INFO Executor: Running task 3.0 in stage 5.0 (TID 15)\n",
            "24/01/15 21:58:12 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "24/01/15 21:58:12 INFO ShuffleBlockFetcherIterator: Getting 2 (160.0 B) non-empty blocks including 2 (160.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "24/01/15 21:58:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:58:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:58:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:58:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:58:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:58:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:58:12 INFO PythonRunner: Times: total = 401, boot = -220, init = 621, finish = 0\n",
            "24/01/15 21:58:12 INFO FileOutputCommitter: Saved output of task 'attempt_202401152158106626524936264339570_0024_m_000003_0' to file:/content/actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/_temporary/0/task_202401152158106626524936264339570_0024_m_000003\n",
            "24/01/15 21:58:12 INFO SparkHadoopMapRedUtil: attempt_202401152158106626524936264339570_0024_m_000003_0: Committed. Elapsed time: 2 ms.\n",
            "24/01/15 21:58:12 INFO Executor: Finished task 3.0 in stage 5.0 (TID 15). 2351 bytes result sent to driver\n",
            "24/01/15 21:58:12 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 16) (03d6364d9dd2, executor driver, partition 0, PROCESS_LOCAL, 7181 bytes) \n",
            "24/01/15 21:58:12 INFO Executor: Running task 0.0 in stage 5.0 (TID 16)\n",
            "24/01/15 21:58:12 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 15) in 503 ms on 03d6364d9dd2 (executor driver) (1/4)\n",
            "24/01/15 21:58:12 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\n",
            "24/01/15 21:58:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:58:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:58:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:58:12 INFO PythonRunner: Times: total = 580, boot = -192, init = 771, finish = 1\n",
            "24/01/15 21:58:12 INFO FileOutputCommitter: Saved output of task 'attempt_202401152158106626524936264339570_0024_m_000001_0' to file:/content/actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/_temporary/0/task_202401152158106626524936264339570_0024_m_000001\n",
            "24/01/15 21:58:12 INFO SparkHadoopMapRedUtil: attempt_202401152158106626524936264339570_0024_m_000001_0: Committed. Elapsed time: 3 ms.\n",
            "24/01/15 21:58:12 INFO Executor: Finished task 1.0 in stage 5.0 (TID 14). 2351 bytes result sent to driver\n",
            "24/01/15 21:58:12 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 17) (03d6364d9dd2, executor driver, partition 2, PROCESS_LOCAL, 7181 bytes) \n",
            "24/01/15 21:58:12 INFO Executor: Running task 2.0 in stage 5.0 (TID 17)\n",
            "24/01/15 21:58:12 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 14) in 680 ms on 03d6364d9dd2 (executor driver) (2/4)\n",
            "24/01/15 21:58:12 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "24/01/15 21:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "24/01/15 21:58:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "24/01/15 21:58:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "24/01/15 21:58:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "24/01/15 21:58:13 INFO PythonRunner: Times: total = 548, boot = -683, init = 1231, finish = 0\n",
            "24/01/15 21:58:13 INFO FileOutputCommitter: Saved output of task 'attempt_202401152158106626524936264339570_0024_m_000000_0' to file:/content/actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/_temporary/0/task_202401152158106626524936264339570_0024_m_000000\n",
            "24/01/15 21:58:13 INFO SparkHadoopMapRedUtil: attempt_202401152158106626524936264339570_0024_m_000000_0: Committed. Elapsed time: 1 ms.\n",
            "24/01/15 21:58:13 INFO Executor: Finished task 0.0 in stage 5.0 (TID 16). 2265 bytes result sent to driver\n",
            "24/01/15 21:58:13 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 16) in 659 ms on 03d6364d9dd2 (executor driver) (3/4)\n",
            "24/01/15 21:58:13 INFO PythonRunner: Times: total = 459, boot = -814, init = 1273, finish = 0\n",
            "24/01/15 21:58:13 INFO FileOutputCommitter: Saved output of task 'attempt_202401152158106626524936264339570_0024_m_000002_0' to file:/content/actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/_temporary/0/task_202401152158106626524936264339570_0024_m_000002\n",
            "24/01/15 21:58:13 INFO SparkHadoopMapRedUtil: attempt_202401152158106626524936264339570_0024_m_000002_0: Committed. Elapsed time: 1 ms.\n",
            "24/01/15 21:58:13 INFO Executor: Finished task 2.0 in stage 5.0 (TID 17). 2265 bytes result sent to driver\n",
            "24/01/15 21:58:13 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 17) in 581 ms on 03d6364d9dd2 (executor driver) (4/4)\n",
            "24/01/15 21:58:13 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "24/01/15 21:58:13 INFO DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:83) finished in 1.323 s\n",
            "24/01/15 21:58:13 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/01/15 21:58:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "24/01/15 21:58:13 INFO DAGScheduler: Job 1 finished: runJob at SparkHadoopWriter.scala:83, took 2.771638 s\n",
            "24/01/15 21:58:13 INFO SparkHadoopWriter: Start to commit write Job job_202401152158106626524936264339570_0024.\n",
            "24/01/15 21:58:13 INFO SparkHadoopWriter: Write Job job_202401152158106626524936264339570_0024 committed. Elapsed time: 47 ms.\n",
            "24/01/15 21:58:13 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "24/01/15 21:58:13 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/01/15 21:58:13 INFO SparkUI: Stopped Spark web UI at http://03d6364d9dd2:4041\n",
            "24/01/15 21:58:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/01/15 21:58:13 INFO MemoryStore: MemoryStore cleared\n",
            "24/01/15 21:58:13 INFO BlockManager: BlockManager stopped\n",
            "24/01/15 21:58:13 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/01/15 21:58:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/01/15 21:58:13 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/01/15 21:58:13 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/01/15 21:58:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-f8d52504-cfcf-48e0-a902-43be522cdf9f/pyspark-21347496-a2f7-4795-8fc3-b9cc3cf943d6\n",
            "24/01/15 21:58:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-f8d52504-cfcf-48e0-a902-43be522cdf9f\n",
            "24/01/15 21:58:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-54346e3d-ed44-40d0-ac52-be237c989491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat actividad2/punto3/data/output/comprasSinTDCMayorDe1500/part-* | sort > actividad2/punto3/data/output/comprasSinTDCMayorDe1500_output.txt"
      ],
      "metadata": {
        "id": "9u4eWPiIkKU1"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/part-* | sort > actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500.txt"
      ],
      "metadata": {
        "id": "I5cMJM43kKSO"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat actividad2/punto3/data/output/comprasSinTDCMayorDe1500_output.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqXftoAokKPo",
        "outputId": "c39533b9-c76b-446c-f117-b9ab224b1197"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Alice', 0)\n",
            "('Bob', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk6zdif4HkD9",
        "outputId": "479babb5-8e5f-475a-f732-aa39eceec708"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Alice', 1)\n",
            "('Bob', 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "jWdQse9wPddB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r actividad2.zip actividad2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5pcpJkpHpVM",
        "outputId": "8829217e-ddd4-4efe-b557-d18a529aa7da"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: actividad2/ (stored 0%)\n",
            "  adding: actividad2/punto3/ (stored 0%)\n",
            "  adding: actividad2/punto3/data/ (stored 0%)\n",
            "  adding: actividad2/punto3/data/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: actividad2/punto3/data/input.txt (deflated 57%)\n",
            "  adding: actividad2/punto3/data/output_a_expected.txt (stored 0%)\n",
            "  adding: actividad2/punto3/data/output_b_expected.txt (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/ (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500.txt (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMayorDe1500_output.txt (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMayorDe1500/ (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMayorDe1500/part-00000 (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMayorDe1500/.part-00001.crc (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMayorDe1500/part-00003 (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMayorDe1500/.part-00002.crc (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMayorDe1500/._SUCCESS.crc (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMayorDe1500/part-00002 (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMayorDe1500/part-00001 (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMayorDe1500/.part-00000.crc (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMayorDe1500/_SUCCESS (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMayorDe1500/.part-00003.crc (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/ (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/part-00000 (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/.part-00001.crc (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/part-00003 (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/.part-00002.crc (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/._SUCCESS.crc (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/part-00002 (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/part-00001 (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/.part-00000.crc (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/_SUCCESS (stored 0%)\n",
            "  adding: actividad2/punto3/data/output/comprasSinTDCMenoroIgualDe1500/.part-00003.crc (stored 0%)\n",
            "  adding: actividad2/punto3/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: actividad2/punto3/src/ (stored 0%)\n",
            "  adding: actividad2/punto3/src/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: actividad2/punto3/src/spark-job.py (deflated 61%)\n",
            "  adding: actividad2/punto2/ (stored 0%)\n",
            "  adding: actividad2/punto2/data/ (stored 0%)\n",
            "  adding: actividad2/punto2/data/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: actividad2/punto2/data/0302/ (stored 0%)\n",
            "  adding: actividad2/punto2/data/0302/2.txt (deflated 51%)\n",
            "  adding: actividad2/punto2/data/0302/1.txt (deflated 55%)\n",
            "  adding: actividad2/punto2/data/0302/0.txt (deflated 42%)\n",
            "  adding: actividad2/punto2/data/0302/log.txt (deflated 22%)\n",
            "  adding: actividad2/punto2/data/0302.zip (stored 0%)\n",
            "  adding: actividad2/punto2/data/output/ (stored 0%)\n",
            "  adding: actividad2/punto2/data/output/output.txt (stored 0%)\n",
            "  adding: actividad2/punto2/data/output/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: actividad2/punto2/data/output/0302_output.txt (stored 0%)\n",
            "  adding: actividad2/punto2/data/input/ (stored 0%)\n",
            "  adding: actividad2/punto2/data/input/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: actividad2/punto2/data/input/0.txt (deflated 69%)\n",
            "  adding: actividad2/punto2/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: actividad2/punto2/src/ (stored 0%)\n",
            "  adding: actividad2/punto2/src/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: actividad2/punto2/src/spark-job.py (deflated 49%)\n",
            "  adding: actividad2/punto1/ (stored 0%)\n",
            "  adding: actividad2/punto1/data/ (stored 0%)\n",
            "  adding: actividad2/punto1/data/output.txt (deflated 6%)\n",
            "  adding: actividad2/punto1/data/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: actividad2/punto1/data/input.txt (deflated 45%)\n",
            "  adding: actividad2/punto1/data/output_expected.txt (stored 0%)\n",
            "  adding: actividad2/punto1/data/output/ (stored 0%)\n",
            "  adding: actividad2/punto1/data/output/part-00000 (stored 0%)\n",
            "  adding: actividad2/punto1/data/output/.part-00001.crc (stored 0%)\n",
            "  adding: actividad2/punto1/data/output/part-00003 (stored 0%)\n",
            "  adding: actividad2/punto1/data/output/.part-00002.crc (stored 0%)\n",
            "  adding: actividad2/punto1/data/output/._SUCCESS.crc (stored 0%)\n",
            "  adding: actividad2/punto1/data/output/part-00002 (stored 0%)\n",
            "  adding: actividad2/punto1/data/output/part-00001 (stored 0%)\n",
            "  adding: actividad2/punto1/data/output/.part-00000.crc (stored 0%)\n",
            "  adding: actividad2/punto1/data/output/_SUCCESS (stored 0%)\n",
            "  adding: actividad2/punto1/data/output/.part-00003.crc (stored 0%)\n",
            "  adding: actividad2/punto1/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: actividad2/punto1/src/ (stored 0%)\n",
            "  adding: actividad2/punto1/src/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: actividad2/punto1/src/spark-job.py (deflated 51%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x2e1t3EPPcLQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}